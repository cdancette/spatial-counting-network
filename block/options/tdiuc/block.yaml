exp:
  dir: logs/tdiuc/debug
  resume: # last, best_[...], or empty (from scratch)
dataset:
  import: block.datasets.factory
  name: tdiuc
  dir: data/vqa/tdiuc
  train_split: train
  eval_split: val # or test
  nb_threads: 4
  batch_size: 64
  nans: 1480
  minwcount: 0
  nlp: mcb
  dir_rcnn: data/vqa/tdiuc/extract_rcnn/2018-04-27_bottom-up-attention_fixed_36
model:
  name: default
  network:
    import: block.models.networks.factory
    name: vqa_net
    txt_enc:
      name: skipthoughts
      type: BayesianUniSkip
      dropout: 0.25
      fixed_emb: False
      dir_st: data/skip-thoughts
    self_q_att: True
    attention: # Glimpse that computes attention over regions with the question representation (output of txt_enc)
      mlp_glimpses: 2
      fusion:
        type: block
        input_dims: [4800, 2048] #[2400, 2048]
        output_dim: 1000
        mm_dim: 1600
        chunks: 18
        rank: 15
        dropout_input: 0.1
        dropout_pre_lin: 0.
    classif:
      fusion:
        type: block
        input_dims: [4800, 4096]
        output_dim: 1480
        mm_dim: 1600
        chunks: 18
        rank: 15
        dropout_input: 0.1
        dropout_pre_lin: 0.
  criterion:
    import: block.models.criterions.factory
    name: vqa_cross_entropy
  metric:
    import: block.models.metrics.factory
    name: vqa_accuracies
optimizer:
  import: block.optimizers.factory
  name: Adamax
  lr: 0.0007
  gradual_warmup_steps: [0.5, 2.0, 4] #torch.linspace
  lr_decay_epochs: [10, 20, 2] #range
  lr_decay_rate: .25
engine:
  name: logger
  debug: False
  print_freq: 10
  nb_epochs: 22
  saving_criteria:
  - eval_epoch.accuracy_top1:max
misc:
  logs_name:
  cuda: True
  seed: 1337
view:
  name: plotly
  items:
  - logs:train_epoch.loss+logs:eval_epoch.loss
  - logs:train_epoch.accuracy_top1+logs:eval_epoch.accuracy_top1
